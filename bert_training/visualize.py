"""
BERT4Rec è¨“ç·´éç¨‹è¦–è¦ºåŒ–å·¥å…·
æä¾› Loss å’Œ Accuracy åœ–è¡¨ç¹ªè£½åŠŸèƒ½
"""

import json
from pathlib import Path
from typing import Dict, List, Optional

import matplotlib.pyplot as plt
import numpy as np


class TrainingVisualizer:
    """è¨“ç·´éç¨‹è¦–è¦ºåŒ–å™¨"""

    def __init__(self, save_dir: Path):
        """
        åˆå§‹åŒ–è¦–è¦ºåŒ–å™¨

        Args:
            save_dir: åœ–è¡¨å„²å­˜ç›®éŒ„
        """
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)

        # è¨­å®šä¸­æ–‡å­—é«”
        plt.rcParams["font.sans-serif"] = [
            "Microsoft YhHei",
            "SimHei",
            "Arial Unicode MS",
        ]
        plt.rcParams["axes.unicode_minus"] = False

        # è¨­å®šæ¨£å¼
        plt.style.use("seaborn-v0_8-darkgrid")

    def plot_loss(
        self,
        train_losses: List[float],
        val_losses: List[float],
        save_name: str = "loss_curve.png",
    ) -> None:
        """
        ç¹ªè£½ Loss æ›²ç·š

        Args:
            train_losses: è¨“ç·´ Loss åˆ—è¡¨
            val_losses: é©—è­‰ Loss åˆ—è¡¨
            save_name: å„²å­˜æª”æ¡ˆåç¨±
        """
        epochs = range(1, len(train_losses) + 1)

        plt.figure(figsize=(12, 6))
        plt.plot(epochs, train_losses, "b-", label="Training Loss", linewidth=2)
        plt.plot(epochs, val_losses, "r-", label="Validation Loss", linewidth=2)

        plt.title("Training and Validation Loss", fontsize=16, fontweight="bold")
        plt.xlabel("Epoch", fontsize=14)
        plt.ylabel("Loss", fontsize=14)
        plt.legend(fontsize=12, loc="best")
        plt.grid(True, alpha=0.3)

        # æ¨™è¨»æœ€å°å€¼
        min_train_idx = np.argmin(train_losses)
        min_val_idx = np.argmin(val_losses)

        plt.plot(min_train_idx + 1, train_losses[min_train_idx], "b*", markersize=15)
        plt.plot(min_val_idx + 1, val_losses[min_val_idx], "r*", markersize=15)

        plt.annotate(
            f"Min: {train_losses[min_train_idx]:.4f}",
            xy=(min_train_idx + 1, train_losses[min_train_idx]),
            xytext=(10, 10),
            textcoords="offset points",
            fontsize=10,
            bbox=dict(boxstyle="round,pad=0.5", fc="blue", alpha=0.3),
        )

        plt.annotate(
            f"Min: {val_losses[min_val_idx]:.4f}",
            xy=(min_val_idx + 1, val_losses[min_val_idx]),
            xytext=(10, -20),
            textcoords="offset points",
            fontsize=10,
            bbox=dict(boxstyle="round,pad=0.5", fc="red", alpha=0.3),
        )

        plt.tight_layout()
        save_path = self.save_dir / save_name
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        plt.close()

        print(f"  ğŸ“Š Loss æ›²ç·šå·²å„²å­˜: {save_path}")

    def plot_accuracy(
        self,
        train_accuracies: Dict[int, List[float]],
        val_accuracies: Dict[int, List[float]],
        save_name: str = "accuracy_curve.png",
    ) -> None:
        """
        ç¹ªè£½ Top-K Accuracy æ›²ç·š

        Args:
            train_accuracies: è¨“ç·´æº–ç¢ºç‡å­—å…¸ {k: [acc1, acc2, ...]}
            val_accuracies: é©—è­‰æº–ç¢ºç‡å­—å…¸ {k: [acc1, acc2, ...]}
            save_name: å„²å­˜æª”æ¡ˆåç¨±
        """
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))

        # è¨“ç·´æº–ç¢ºç‡
        ax1 = axes[0]
        for k in sorted(train_accuracies.keys()):
            epochs = range(1, len(train_accuracies[k]) + 1)
            ax1.plot(
                epochs,
                train_accuracies[k],
                label=f"Top-{k}",
                linewidth=2,
                marker="o",
                markersize=3,
            )

        ax1.set_title("Training Accuracy", fontsize=16, fontweight="bold")
        ax1.set_xlabel("Epoch", fontsize=14)
        ax1.set_ylabel("Accuracy", fontsize=14)
        ax1.legend(fontsize=12, loc="best")
        ax1.grid(True, alpha=0.3)

        # é©—è­‰æº–ç¢ºç‡
        ax2 = axes[1]
        for k in sorted(val_accuracies.keys()):
            epochs = range(1, len(val_accuracies[k]) + 1)
            ax2.plot(
                epochs,
                val_accuracies[k],
                label=f"Top-{k}",
                linewidth=2,
                marker="o",
                markersize=3,
            )

        ax2.set_title("Validation Accuracy", fontsize=16, fontweight="bold")
        ax2.set_xlabel("Epoch", fontsize=14)
        ax2.set_ylabel("Accuracy", fontsize=14)
        ax2.legend(fontsize=12, loc="best")
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        save_path = self.save_dir / save_name
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        plt.close()

        print(f"  ğŸ“Š Accuracy æ›²ç·šå·²å„²å­˜: {save_path}")

    def plot_combined_metrics(
        self,
        train_losses: List[float],
        val_losses: List[float],
        train_accuracies: Dict[int, List[float]],
        val_accuracies: Dict[int, List[float]],
        save_name: str = "combined_metrics.png",
    ) -> None:
        """
        ç¹ªè£½ç¶œåˆæŒ‡æ¨™åœ–ï¼ˆLoss + Accuracyï¼‰

        Args:
            train_losses: è¨“ç·´ Loss åˆ—è¡¨
            val_losses: é©—è­‰ Loss åˆ—è¡¨
            train_accuracies: è¨“ç·´æº–ç¢ºç‡å­—å…¸
            val_accuracies: é©—è­‰æº–ç¢ºç‡å­—å…¸
            save_name: å„²å­˜æª”æ¡ˆåç¨±
        """
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))

        epochs = range(1, len(train_losses) + 1)

        # 1. Loss æ›²ç·š
        ax1 = axes[0, 0]
        ax1.plot(epochs, train_losses, "b-", label="Training Loss", linewidth=2)
        ax1.plot(epochs, val_losses, "r-", label="Validation Loss", linewidth=2)
        ax1.set_title("Loss Curves", fontsize=14, fontweight="bold")
        ax1.set_xlabel("Epoch", fontsize=12)
        ax1.set_ylabel("Loss", fontsize=12)
        ax1.legend(fontsize=10)
        ax1.grid(True, alpha=0.3)

        # 2. è¨“ç·´æº–ç¢ºç‡
        ax2 = axes[0, 1]
        for k in sorted(train_accuracies.keys()):
            ax2.plot(
                epochs,
                train_accuracies[k],
                label=f"Top-{k}",
                linewidth=2,
                marker="o",
                markersize=2,
            )
        ax2.set_title("Training Accuracy", fontsize=14, fontweight="bold")
        ax2.set_xlabel("Epoch", fontsize=12)
        ax2.set_ylabel("Accuracy", fontsize=12)
        ax2.legend(fontsize=10)
        ax2.grid(True, alpha=0.3)

        # 3. é©—è­‰æº–ç¢ºç‡
        ax3 = axes[1, 0]
        for k in sorted(val_accuracies.keys()):
            ax3.plot(
                epochs,
                val_accuracies[k],
                label=f"Top-{k}",
                linewidth=2,
                marker="o",
                markersize=2,
            )
        ax3.set_title("Validation Accuracy", fontsize=14, fontweight="bold")
        ax3.set_xlabel("Epoch", fontsize=12)
        ax3.set_ylabel("Accuracy", fontsize=12)
        ax3.legend(fontsize=10)
        ax3.grid(True, alpha=0.3)

        # 4. æœ€ä½³ Top-K æ¯”è¼ƒ
        ax4 = axes[1, 1]
        k_values = sorted(train_accuracies.keys())
        train_best = [max(train_accuracies[k]) for k in k_values]
        val_best = [max(val_accuracies[k]) for k in k_values]

        x = np.arange(len(k_values))
        width = 0.35

        ax4.bar(x - width / 2, train_best, width, label="Training", alpha=0.8)
        ax4.bar(x + width / 2, val_best, width, label="Validation", alpha=0.8)

        ax4.set_title("Best Top-K Accuracy Comparison", fontsize=14, fontweight="bold")
        ax4.set_xlabel("Top-K", fontsize=12)
        ax4.set_ylabel("Accuracy", fontsize=12)
        ax4.set_xticks(x)
        ax4.set_xticklabels([f"Top-{k}" for k in k_values])
        ax4.legend(fontsize=10)
        ax4.grid(True, alpha=0.3, axis="y")

        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for i, (train_acc, val_acc) in enumerate(zip(train_best, val_best)):
            ax4.text(
                i - width / 2,
                train_acc,
                f"{train_acc:.3f}",
                ha="center",
                va="bottom",
                fontsize=8,
            )
            ax4.text(
                i + width / 2,
                val_acc,
                f"{val_acc:.3f}",
                ha="center",
                va="bottom",
                fontsize=8,
            )

        plt.tight_layout()
        save_path = self.save_dir / save_name
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        plt.close()

        print(f"  ğŸ“Š ç¶œåˆæŒ‡æ¨™åœ–å·²å„²å­˜: {save_path}")

    def plot_learning_curve(
        self,
        train_losses: List[float],
        val_losses: List[float],
        save_name: str = "learning_curve.png",
    ) -> None:
        """
        ç¹ªè£½å­¸ç¿’æ›²ç·šï¼ˆåŒ…å«å¹³æ»‘æ›²ç·šï¼‰

        Args:
            train_losses: è¨“ç·´ Loss åˆ—è¡¨
            val_losses: é©—è­‰ Loss åˆ—è¡¨
            save_name: å„²å­˜æª”æ¡ˆåç¨±
        """
        epochs = range(1, len(train_losses) + 1)

        # è¨ˆç®—ç§»å‹•å¹³å‡
        def moving_average(data, window=10):
            return np.convolve(data, np.ones(window) / window, mode="valid")

        window_size = min(10, len(train_losses) // 10 + 1)
        train_smooth = moving_average(train_losses, window_size)
        val_smooth = moving_average(val_losses, window_size)

        plt.figure(figsize=(14, 7))

        # åŸå§‹æ›²ç·šï¼ˆåŠé€æ˜ï¼‰
        plt.plot(epochs, train_losses, "b-", alpha=0.3, linewidth=1)
        plt.plot(epochs, val_losses, "r-", alpha=0.3, linewidth=1)

        # å¹³æ»‘æ›²ç·š
        smooth_epochs = range(1, len(train_smooth) + 1)
        plt.plot(
            smooth_epochs,
            train_smooth,
            "b-",
            label="Training Loss (smoothed)",
            linewidth=2.5,
        )
        plt.plot(
            smooth_epochs,
            val_smooth,
            "r-",
            label="Validation Loss (smoothed)",
            linewidth=2.5,
        )

        plt.title("Learning Curve (Smoothed)", fontsize=16, fontweight="bold")
        plt.xlabel("Epoch", fontsize=14)
        plt.ylabel("Loss", fontsize=14)
        plt.legend(fontsize=12, loc="best")
        plt.grid(True, alpha=0.3)

        plt.tight_layout()
        save_path = self.save_dir / save_name
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        plt.close()

        print(f"  ğŸ“Š å­¸ç¿’æ›²ç·šå·²å„²å­˜: {save_path}")

    def save_metrics_json(
        self,
        train_losses: List[float],
        val_losses: List[float],
        train_accuracies: Dict[int, List[float]],
        val_accuracies: Dict[int, List[float]],
        save_name: str = "training_metrics.json",
    ) -> None:
        """
        å„²å­˜è¨“ç·´æŒ‡æ¨™ç‚º JSON æ ¼å¼

        Args:
            train_losses: è¨“ç·´ Loss åˆ—è¡¨
            val_losses: é©—è­‰ Loss åˆ—è¡¨
            train_accuracies: è¨“ç·´æº–ç¢ºç‡å­—å…¸
            val_accuracies: é©—è­‰æº–ç¢ºç‡å­—å…¸
            save_name: å„²å­˜æª”æ¡ˆåç¨±
        """
        metrics = {
            "train_losses": train_losses,
            "val_losses": val_losses,
            "train_accuracies": {str(k): v for k, v in train_accuracies.items()},
            "val_accuracies": {str(k): v for k, v in val_accuracies.items()},
            "summary": {
                "best_train_loss": float(min(train_losses)),
                "best_val_loss": float(min(val_losses)),
                "final_train_loss": float(train_losses[-1]),
                "final_val_loss": float(val_losses[-1]),
                "best_train_acc": {
                    k: float(max(v)) for k, v in train_accuracies.items()
                },
                "best_val_acc": {k: float(max(v)) for k, v in val_accuracies.items()},
            },
        }

        save_path = self.save_dir / save_name
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)

        print(f"  ğŸ’¾ è¨“ç·´æŒ‡æ¨™å·²å„²å­˜: {save_path}")

    def plot_all(
        self,
        train_losses: List[float],
        val_losses: List[float],
        train_accuracies: Dict[int, List[float]],
        val_accuracies: Dict[int, List[float]],
    ) -> None:
        """
        ç¹ªè£½æ‰€æœ‰åœ–è¡¨

        Args:
            train_losses: è¨“ç·´ Loss åˆ—è¡¨
            val_losses: é©—è­‰ Loss åˆ—è¡¨
            train_accuracies: è¨“ç·´æº–ç¢ºç‡å­—å…¸
            val_accuracies: é©—è­‰æº–ç¢ºç‡å­—å…¸
        """
        print("\nğŸ“Š æ­£åœ¨ç”Ÿæˆè¨“ç·´è¦–è¦ºåŒ–åœ–è¡¨...")

        self.plot_loss(train_losses, val_losses)
        self.plot_accuracy(train_accuracies, val_accuracies)
        self.plot_combined_metrics(
            train_losses, val_losses, train_accuracies, val_accuracies
        )
        self.plot_learning_curve(train_losses, val_losses)
        self.save_metrics_json(
            train_losses, val_losses, train_accuracies, val_accuracies
        )

        print("  âœ… æ‰€æœ‰åœ–è¡¨ç”Ÿæˆå®Œæˆï¼")


def plot_final_results(
    train_losses: List[float],
    val_losses: List[float],
    train_accuracies: Dict[int, List[float]],
    val_accuracies: Dict[int, List[float]],
    save_dir: Path,
) -> None:
    """
    ç¹ªè£½æœ€çµ‚çµæœï¼ˆä¾¿æ·å‡½æ•¸ï¼‰

    Args:
        train_losses: è¨“ç·´ Loss åˆ—è¡¨
        val_losses: é©—è­‰ Loss åˆ—è¡¨
        train_accuracies: è¨“ç·´æº–ç¢ºç‡å­—å…¸
        val_accuracies: é©—è­‰æº–ç¢ºç‡å­—å…¸
        save_dir: å„²å­˜ç›®éŒ„
    """
    visualizer = TrainingVisualizer(save_dir)
    visualizer.plot_all(train_losses, val_losses, train_accuracies, val_accuracies)
