"""
BERT4Rec å®Œæ•´è¨“ç·´è…³æœ¬
åŒ…å« Loss å’Œ Top-K Accuracy è¨ˆç®—åŠè¦–è¦ºåŒ–

ä½¿ç”¨æ–¹å¼:
    python train_model.py
    python train_model.py --epochs 200 --batch-size 128
    python train_model.py --gpu --fp16
"""

import argparse
import logging
import pickle
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from sqlmodel import Session, create_engine, select
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

# ä¿®æ­£ Windows ç·¨ç¢¼å•é¡Œ
if sys.platform == "win32":
    import codecs

    if hasattr(sys.stdout, "buffer"):
        sys.stdout = codecs.getwriter("utf-8")(sys.stdout.buffer, "strict")
        sys.stderr = codecs.getwriter("utf-8")(sys.stderr.buffer, "strict")

# å°å…¥é…ç½®å’Œè¦–è¦ºåŒ–
from config import Config
from visualize import TrainingVisualizer

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("training.log", encoding="utf-8"),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)


# ============================================================================
# è³‡æ–™åº«æ¨¡å‹ï¼ˆç°¡åŒ–ç‰ˆï¼Œé¿å…é‡è¤‡å°å…¥ï¼‰
# ============================================================================

from sqlmodel import Field, SQLModel


class BERTAnime(SQLModel, table=True):
    """å‹•ç•«è³‡æ–™è¡¨"""

    __tablename__ = "bert_anime"
    id: int = Field(primary_key=True)
    title_romaji: str
    title_english: str | None = None
    title_native: str | None = None


class BERTUserAnimeList(SQLModel, table=True):
    """ä½¿ç”¨è€…å‹•ç•«åˆ—è¡¨"""

    __tablename__ = "bert_user_anime_list"
    id: int | None = Field(default=None, primary_key=True)
    user_id: int
    username: str
    anime_id: int
    status: str
    score: float = 0.0
    progress: int = 0


# ============================================================================
# BERT4Rec æ¨¡å‹
# ============================================================================


class BERT4Rec(nn.Module):
    """BERT4Rec æ¨è–¦æ¨¡å‹"""

    def __init__(
        self,
        num_items: int,
        max_seq_len: int = 200,
        hidden_size: int = 256,
        num_layers: int = 2,
        num_heads: int = 4,
        dropout: float = 0.1,
    ):
        super().__init__()

        self.num_items = num_items
        self.max_seq_len = max_seq_len
        self.hidden_size = hidden_size

        # Token embeddings (+3 for PAD, MASK, CLS)
        self.item_embedding = nn.Embedding(num_items + 3, hidden_size, padding_idx=0)
        self.position_embedding = nn.Embedding(max_seq_len, hidden_size)

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_size,
            nhead=num_heads,
            dim_feedforward=hidden_size * 4,
            dropout=dropout,
            batch_first=True,
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # Output layer
        self.out = nn.Linear(hidden_size, num_items + 3)

        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(hidden_size)

        # Special tokens
        self.pad_token = 0
        self.mask_token = num_items + 1
        self.cls_token = num_items + 2

    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None):
        batch_size, seq_len = input_ids.size()

        # Embeddings
        item_emb = self.item_embedding(input_ids)
        position_ids = torch.arange(seq_len, device=input_ids.device)
        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)
        position_emb = self.position_embedding(position_ids)

        embeddings = item_emb + position_emb
        embeddings = self.layer_norm(embeddings)
        embeddings = self.dropout(embeddings)

        # Attention mask
        if attention_mask is None:
            attention_mask = (input_ids != self.pad_token).float()

        # Transformer
        hidden_states = self.transformer(
            embeddings, src_key_padding_mask=(attention_mask == 0)
        )

        # Output
        logits = self.out(hidden_states)
        return logits


# ============================================================================
# è³‡æ–™é›†
# ============================================================================


class BERT4RecDataset(Dataset):
    """BERT4Rec è¨“ç·´è³‡æ–™é›†"""

    def __init__(
        self,
        user_sequences: List[List[int]],
        item_to_idx: Dict[int, int],
        max_seq_len: int = 200,
        mask_prob: float = 0.15,
        mask_token: int = None,
    ):
        self.user_sequences = user_sequences
        self.item_to_idx = item_to_idx
        self.max_seq_len = max_seq_len
        self.mask_prob = mask_prob
        self.mask_token = mask_token
        self.pad_token = 0

    def __len__(self):
        return len(self.user_sequences)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        sequence = self.user_sequences[idx]
        sequence = [
            self.item_to_idx[item] for item in sequence if item in self.item_to_idx
        ]

        if not sequence:
            sequence = [self.pad_token]

        # æˆªæ–·æˆ–å¡«å……
        if len(sequence) > self.max_seq_len:
            sequence = sequence[-self.max_seq_len :]
        else:
            sequence = [self.pad_token] * (self.max_seq_len - len(sequence)) + sequence

        labels = torch.tensor(sequence, dtype=torch.long)
        input_ids = labels.clone()
        attention_mask = (input_ids != self.pad_token).long()

        # éš¨æ©Ÿé®ç½©
        mask_positions = torch.rand(self.max_seq_len) < self.mask_prob
        mask_positions = mask_positions & (input_ids != self.pad_token)

        if self.mask_token is not None:
            input_ids[mask_positions] = self.mask_token

        return input_ids, labels, attention_mask


# ============================================================================
# è¨“ç·´å™¨ï¼ˆå«æº–ç¢ºç‡è¨ˆç®—ï¼‰
# ============================================================================


class BERT4RecTrainer:
    """BERT4Rec è¨“ç·´å™¨ï¼ˆå« Top-K Accuracyï¼‰"""

    def __init__(
        self,
        model: BERT4Rec,
        device: torch.device,
        learning_rate: float = 1e-3,
        use_fp16: bool = False,
        top_k_list: List[int] = None,
    ):
        self.model = model.to(device)
        self.device = device
        self.use_fp16 = use_fp16
        self.top_k_list = top_k_list or [1, 5, 10, 20]

        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        self.criterion = nn.CrossEntropyLoss(ignore_index=0, reduction="mean")

        # FP16 training
        if use_fp16 and device.type == "cuda":
            self.scaler = torch.cuda.amp.GradScaler()
            logger.info("âœ… å•Ÿç”¨ FP16 è¨“ç·´")
        else:
            self.scaler = None

        # è¨˜éŒ„æŒ‡æ¨™
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = {k: [] for k in self.top_k_list}
        self.val_accuracies = {k: [] for k in self.top_k_list}

    def calculate_top_k_accuracy(
        self, logits: torch.Tensor, labels: torch.Tensor, attention_mask: torch.Tensor
    ) -> Dict[int, float]:
        """
        è¨ˆç®— Top-K æº–ç¢ºç‡

        Args:
            logits: [batch_size, seq_len, num_items]
            labels: [batch_size, seq_len]
            attention_mask: [batch_size, seq_len]

        Returns:
            {k: accuracy} å­—å…¸
        """
        batch_size, seq_len, num_items = logits.size()

        # åªè¨ˆç®—é padding ä½ç½®
        mask = attention_mask.bool()

        # ç²å–é æ¸¬çš„ top-k é …ç›®
        _, top_k_preds = torch.topk(logits, max(self.top_k_list), dim=-1)

        accuracies = {}
        for k in self.top_k_list:
            # æª¢æŸ¥çœŸå¯¦æ¨™ç±¤æ˜¯å¦åœ¨ top-k é æ¸¬ä¸­
            top_k = top_k_preds[:, :, :k]  # [batch_size, seq_len, k]
            labels_expanded = labels.unsqueeze(-1).expand_as(
                top_k
            )  # [batch_size, seq_len, k]

            # æª¢æŸ¥æ˜¯å¦åŒ¹é…
            correct = (top_k == labels_expanded).any(dim=-1)  # [batch_size, seq_len]

            # åªè¨ˆç®—æœ‰æ•ˆä½ç½®
            correct = correct & mask
            accuracy = correct.sum().float() / mask.sum().float()
            accuracies[k] = accuracy.item()

        return accuracies

    def train_epoch(self, dataloader: DataLoader) -> Tuple[float, Dict[int, float]]:
        """è¨“ç·´ä¸€å€‹ epoch"""
        self.model.train()
        total_loss = 0
        num_batches = 0
        epoch_accuracies = {k: 0.0 for k in self.top_k_list}

        progress_bar = tqdm(dataloader, desc="è¨“ç·´ä¸­", unit="batch")
        for input_ids, labels, attention_mask in progress_bar:
            input_ids = input_ids.to(self.device)
            labels = labels.to(self.device)
            attention_mask = attention_mask.to(self.device)

            self.optimizer.zero_grad()

            # Forward pass
            if self.use_fp16 and self.scaler:
                with torch.cuda.amp.autocast():
                    logits = self.model(input_ids, attention_mask)
                    logits_2d = logits.view(-1, logits.size(-1))
                    labels_1d = labels.view(-1)
                    loss = self.criterion(logits_2d, labels_1d)

                self.scaler.scale(loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                logits = self.model(input_ids, attention_mask)
                logits_2d = logits.view(-1, logits.size(-1))
                labels_1d = labels.view(-1)
                loss = self.criterion(logits_2d, labels_1d)

                loss.backward()
                self.optimizer.step()

            # è¨ˆç®—æº–ç¢ºç‡
            with torch.no_grad():
                batch_acc = self.calculate_top_k_accuracy(
                    logits, labels, attention_mask
                )
                for k in self.top_k_list:
                    epoch_accuracies[k] += batch_acc[k]

            total_loss += loss.item()
            num_batches += 1

            progress_bar.set_postfix(
                {"loss": f"{loss.item():.4f}", "top1_acc": f"{batch_acc[1]:.4f}"}
            )

        avg_loss = total_loss / num_batches
        avg_accuracies = {k: v / num_batches for k, v in epoch_accuracies.items()}

        self.train_losses.append(avg_loss)
        for k in self.top_k_list:
            self.train_accuracies[k].append(avg_accuracies[k])

        return avg_loss, avg_accuracies

    @torch.no_grad()
    def validate(self, dataloader: DataLoader) -> Tuple[float, Dict[int, float]]:
        """é©—è­‰"""
        self.model.eval()
        total_loss = 0
        num_batches = 0
        epoch_accuracies = {k: 0.0 for k in self.top_k_list}

        for input_ids, labels, attention_mask in tqdm(
            dataloader, desc="é©—è­‰ä¸­", unit="batch"
        ):
            input_ids = input_ids.to(self.device)
            labels = labels.to(self.device)
            attention_mask = attention_mask.to(self.device)

            logits = self.model(input_ids, attention_mask)
            logits_2d = logits.view(-1, logits.size(-1))
            labels_1d = labels.view(-1)
            loss = self.criterion(logits_2d, labels_1d)

            # è¨ˆç®—æº–ç¢ºç‡
            batch_acc = self.calculate_top_k_accuracy(logits, labels, attention_mask)
            for k in self.top_k_list:
                epoch_accuracies[k] += batch_acc[k]

            total_loss += loss.item()
            num_batches += 1

        avg_loss = total_loss / num_batches
        avg_accuracies = {k: v / num_batches for k, v in epoch_accuracies.items()}

        self.val_losses.append(avg_loss)
        for k in self.top_k_list:
            self.val_accuracies[k].append(avg_accuracies[k])

        return avg_loss, avg_accuracies

    def save_checkpoint(self, epoch: int, filepath: Path) -> None:
        """å„²å­˜ checkpoint"""
        checkpoint = {
            "epoch": epoch,
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "train_losses": self.train_losses,
            "val_losses": self.val_losses,
            "train_accuracies": self.train_accuracies,
            "val_accuracies": self.val_accuracies,
            "model_config": {
                "num_items": self.model.num_items,
                "max_seq_len": self.model.max_seq_len,
                "hidden_size": self.model.hidden_size,
            },
        }
        torch.save(checkpoint, filepath)
        logger.info(f"ğŸ’¾ Checkpoint å·²å„²å­˜: {filepath}")


# ============================================================================
# è³‡æ–™è¼‰å…¥
# ============================================================================


def load_dataset_from_db(db_path: Path) -> Tuple[List[List[int]], Dict, Dict, int]:
    """å¾è³‡æ–™åº«è¼‰å…¥è³‡æ–™é›†"""
    print("\nğŸ“š è¼‰å…¥è³‡æ–™é›†...")

    engine = create_engine(f"sqlite:///{db_path}", echo=False)

    with Session(engine) as session:
        # è¼‰å…¥æ‰€æœ‰å‹•ç•«
        animes = session.exec(select(BERTAnime)).all()
        anime_ids = sorted([anime.id for anime in animes])

        # å»ºç«‹æ˜ å°„
        item_to_idx = {anime_id: idx + 1 for idx, anime_id in enumerate(anime_ids)}
        idx_to_item = {idx: anime_id for anime_id, idx in item_to_idx.items()}
        num_items = len(anime_ids)

        print(f"  âœ“ è¼‰å…¥ {num_items} éƒ¨å‹•ç•«")

        # è¼‰å…¥ä½¿ç”¨è€…åºåˆ—
        user_lists = session.exec(select(BERTUserAnimeList)).all()

        # æŒ‰ä½¿ç”¨è€…åˆ†çµ„
        user_sequences_dict = {}
        for entry in user_lists:
            if entry.user_id not in user_sequences_dict:
                user_sequences_dict[entry.user_id] = []
            user_sequences_dict[entry.user_id].append(entry.anime_id)

        user_sequences = list(user_sequences_dict.values())
        print(f"  âœ“ è¼‰å…¥ {len(user_sequences)} å€‹ä½¿ç”¨è€…åºåˆ—")

    return user_sequences, item_to_idx, idx_to_item, num_items


def split_dataset(
    user_sequences: List[List[int]], val_ratio: float = 0.1
) -> Tuple[List[List[int]], List[List[int]]]:
    """åˆ†å‰²è¨“ç·´é›†å’Œé©—è­‰é›†"""
    num_val = int(len(user_sequences) * val_ratio)
    indices = np.random.permutation(len(user_sequences))

    val_indices = indices[:num_val]
    train_indices = indices[num_val:]

    train_sequences = [user_sequences[i] for i in train_indices]
    val_sequences = [user_sequences[i] for i in val_indices]

    return train_sequences, val_sequences


# ============================================================================
# ä¸»è¨“ç·´æµç¨‹
# ============================================================================


def main():
    parser = argparse.ArgumentParser(
        description="è¨“ç·´ BERT4Rec æ¨è–¦æ¨¡å‹",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    # è¨“ç·´åƒæ•¸
    parser.add_argument("--epochs", type=int, default=200, help="è¨“ç·´è¼ªæ•¸ (é è¨­: 200)")
    parser.add_argument(
        "--batch-size", type=int, default=64, help="æ‰¹æ¬¡å¤§å° (é è¨­: 64)"
    )
    parser.add_argument("--lr", type=float, default=1e-3, help="å­¸ç¿’ç‡ (é è¨­: 0.001)")
    parser.add_argument(
        "--val-ratio", type=float, default=0.1, help="é©—è­‰é›†æ¯”ä¾‹ (é è¨­: 0.1)"
    )

    # æ¨¡å‹åƒæ•¸
    parser.add_argument(
        "--hidden-size", type=int, default=256, help="éš±è—å±¤å¤§å° (é è¨­: 256)"
    )
    parser.add_argument(
        "--num-layers", type=int, default=2, help="Transformer å±¤æ•¸ (é è¨­: 2)"
    )
    parser.add_argument("--num-heads", type=int, default=4, help="æ³¨æ„åŠ›é ­æ•¸ (é è¨­: 4)")
    parser.add_argument(
        "--max-seq-len", type=int, default=200, help="æœ€å¤§åºåˆ—é•·åº¦ (é è¨­: 200)"
    )
    parser.add_argument(
        "--dropout", type=float, default=0.1, help="Dropout (é è¨­: 0.1)"
    )

    # ç¡¬é«”
    parser.add_argument("--gpu", action="store_true", help="ä½¿ç”¨ GPU")
    parser.add_argument("--fp16", action="store_true", help="ä½¿ç”¨ FP16 è¨“ç·´")

    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42, help="éš¨æ©Ÿç¨®å­ (é è¨­: 42)")

    args = parser.parse_args()

    # åˆå§‹åŒ–é…ç½®
    config = Config()
    config.update_from_args(args)
    config.print_config()

    # è¨­å®šéš¨æ©Ÿç¨®å­
    torch.manual_seed(config.training.random_seed)
    np.random.seed(config.training.random_seed)

    # è¨­å®šè£ç½®
    if config.training.use_gpu and torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"\nâœ… ä½¿ç”¨ GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = torch.device("cpu")
        print("\nâš ï¸  ä½¿ç”¨ CPU")

    try:
        # è¼‰å…¥è³‡æ–™
        user_sequences, item_to_idx, idx_to_item, num_items = load_dataset_from_db(
            config.paths.db_path
        )

        if len(user_sequences) == 0:
            print("\nâŒ éŒ¯èª¤: æ²’æœ‰å¯ç”¨çš„è¨“ç·´è³‡æ–™")
            print("è«‹å…ˆåŸ·è¡Œæº–å‚™è³‡æ–™çš„æ­¥é©Ÿ")
            sys.exit(1)

        # åˆ†å‰²è³‡æ–™
        train_sequences, val_sequences = split_dataset(
            user_sequences, val_ratio=config.training.val_ratio
        )
        print(f"\n  è¨“ç·´é›†: {len(train_sequences)} å€‹åºåˆ—")
        print(f"  é©—è­‰é›†: {len(val_sequences)} å€‹åºåˆ—")

        # å»ºç«‹è³‡æ–™é›†
        mask_token = num_items + 1
        train_dataset = BERT4RecDataset(
            train_sequences,
            item_to_idx,
            config.training.max_seq_len,
            mask_token=mask_token,
        )
        val_dataset = BERT4RecDataset(
            val_sequences,
            item_to_idx,
            config.training.max_seq_len,
            mask_token=mask_token,
        )

        # å»ºç«‹ DataLoader
        train_loader = DataLoader(
            train_dataset,
            batch_size=config.training.batch_size,
            shuffle=True,
            num_workers=config.training.num_workers,
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=config.training.batch_size,
            shuffle=False,
            num_workers=config.training.num_workers,
        )

        # å»ºç«‹æ¨¡å‹
        model = BERT4Rec(
            num_items=num_items,
            max_seq_len=config.training.max_seq_len,
            hidden_size=config.training.hidden_size,
            num_layers=config.training.num_layers,
            num_heads=config.training.num_heads,
            dropout=config.training.dropout,
        )

        total_params = sum(p.numel() for p in model.parameters())
        print(f"\nâœ… æ¨¡å‹å»ºç«‹å®Œæˆ")
        print(f"  ç¸½åƒæ•¸é‡: {total_params:,}")

        # å»ºç«‹è¨“ç·´å™¨
        trainer = BERT4RecTrainer(
            model,
            device,
            learning_rate=config.training.learning_rate,
            use_fp16=config.training.use_fp16 and config.training.use_gpu,
            top_k_list=config.data.top_k_list,
        )

        # åˆå§‹åŒ–è¦–è¦ºåŒ–å™¨
        visualizer = TrainingVisualizer(config.paths.plot_dir)

        # è¨“ç·´
        print("\n" + "=" * 80)
        print("ğŸ¯ é–‹å§‹è¨“ç·´")
        print("=" * 80)

        best_val_loss = float("inf")
        best_val_acc = 0.0

        for epoch in range(1, config.training.epochs + 1):
            print(f"\nğŸ“‹ Epoch {epoch}/{config.training.epochs}")
            print("-" * 80)

            # è¨“ç·´
            train_loss, train_acc = trainer.train_epoch(train_loader)
            print(f"  è¨“ç·´ Loss: {train_loss:.4f}")
            print(f"  è¨“ç·´ Top-1 Acc: {train_acc[1]:.4f}")
            print(f"  è¨“ç·´ Top-5 Acc: {train_acc[5]:.4f}")
            print(f"  è¨“ç·´ Top-10 Acc: {train_acc[10]:.4f}")

            # é©—è­‰
            val_loss, val_acc = trainer.validate(val_loader)
            print(f"  é©—è­‰ Loss: {val_loss:.4f}")
            print(f"  é©—è­‰ Top-1 Acc: {val_acc[1]:.4f}")
            print(f"  é©—è­‰ Top-5 Acc: {val_acc[5]:.4f}")
            print(f"  é©—è­‰ Top-10 Acc: {val_acc[10]:.4f}")

            # å„²å­˜ checkpoint
            if epoch % config.training.save_every_n_epochs == 0:
                checkpoint_path = (
                    config.paths.checkpoint_dir / f"checkpoint_epoch_{epoch}.pth"
                )
                trainer.save_checkpoint(epoch, checkpoint_path)

            # å„²å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_val_acc = val_acc[1]
                best_model_path = config.paths.model_dir / "best_model.pth"
                trainer.save_checkpoint(epoch, best_model_path)
                print(f"  ğŸŒŸ æ–°çš„æœ€ä½³æ¨¡å‹ï¼")

        # è¨“ç·´å®Œæˆ
        print("\n" + "=" * 80)
        print("ğŸ‰ è¨“ç·´å®Œæˆï¼")
        print("=" * 80)
        print(f"  æœ€ä½³é©—è­‰ Loss: {best_val_loss:.4f}")
        print(f"  æœ€ä½³é©—è­‰ Top-1 Acc: {best_val_acc:.4f}")

        # å„²å­˜æœ€çµ‚æ¨¡å‹
        final_model_path = config.paths.model_dir / "final_model.pth"
        trainer.save_checkpoint(config.training.epochs, final_model_path)

        # å„²å­˜æ˜ å°„è³‡æ–™
        mapping_path = config.paths.model_dir / "item_mappings.pkl"
        with open(mapping_path, "wb") as f:
            pickle.dump(
                {
                    "item_to_idx": item_to_idx,
                    "idx_to_item": idx_to_item,
                    "num_items": num_items,
                },
                f,
            )
        print(f"  æ˜ å°„è³‡æ–™å·²å„²å­˜: {mapping_path}")

        # å„²å­˜é…ç½®
        config_path = config.paths.model_dir / "training_config.json"
        config.save_to_file(config_path)
        print(f"  é…ç½®å·²å„²å­˜: {config_path}")

        # ç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨
        visualizer.plot_all(
            trainer.train_losses,
            trainer.val_losses,
            trainer.train_accuracies,
            trainer.val_accuracies,
        )

        print("\nâœ… æ‰€æœ‰è¼¸å‡ºå·²å„²å­˜è‡³:")
        print(f"  æ¨¡å‹: {config.paths.model_dir}")
        print(f"  åœ–è¡¨: {config.paths.plot_dir}")
        print(f"  æ—¥èªŒ: {config.paths.log_dir}")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  è¨“ç·´è¢«ä¸­æ–·")
        sys.exit(130)
    except Exception as e:
        logger.error(f"è¨“ç·´å¤±æ•—: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
