"""
å„ªåŒ–ç‰ˆ BERT æ¨è–¦å™¨
æ”¯æ´å¿«å–ã€æ‰¹æ¬¡è™•ç†ã€GPU åŠ é€Ÿ
"""

import hashlib
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import torch
import torch.nn as nn
from sqlmodel import Session, select
from tqdm import tqdm

from models import BERTRecommendationCache, BERTUserProfile

logger = logging.getLogger(__name__)


class OptimizedBERTRecommender:
    """
    å„ªåŒ–ç‰ˆ BERT æ¨è–¦å™¨
    - å¿«å–ä½¿ç”¨è€… Profile å’Œæ¨è–¦çµæœ
    - æ‰¹æ¬¡è™•ç†æå‡æ•ˆèƒ½
    - GPU åŠ é€Ÿæ”¯æ´
    - å¢é‡æ›´æ–°æ©Ÿåˆ¶
    """

    def __init__(
        self,
        model_path: Optional[str] = None,
        dataset_path: Optional[str] = None,
        anime_metadata_path: Optional[str] = None,
        device: str = "auto",
        use_fp16: bool = False,
        batch_size: int = 32,
        cache_expiry_days: int = 7,
        db_session: Optional[Session] = None,
    ):
        """
        åˆå§‹åŒ–å„ªåŒ–ç‰ˆ BERT æ¨è–¦å™¨

        Args:
            model_path: é è¨“ç·´æ¨¡å‹è·¯å¾‘
            dataset_path: è³‡æ–™é›†è·¯å¾‘
            anime_metadata_path: å‹•ç•« metadata è·¯å¾‘
            device: é‹ç®—è¨­å‚™ ('auto', 'cpu', 'cuda')
            use_fp16: æ˜¯å¦ä½¿ç”¨ FP16 åŠ é€Ÿï¼ˆåƒ… GPUï¼‰
            batch_size: æ‰¹æ¬¡è™•ç†å¤§å°
            cache_expiry_days: å¿«å–éæœŸå¤©æ•¸
            db_session: è³‡æ–™åº« session
        """
        # è‡ªå‹•é¸æ“‡è¨­å‚™
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)

        self.use_fp16 = use_fp16 and self.device.type == "cuda"
        self.batch_size = batch_size
        self.cache_expiry_days = cache_expiry_days
        self.db_session = db_session

        # æ¨¡å‹å’Œè³‡æ–™
        self.model = None
        self.dataset = None
        self.anime_metadata = {}
        self.id_mapping = {}
        self.reverse_id_mapping = {}

        # æ•ˆèƒ½çµ±è¨ˆ
        self.stats = {
            "cache_hits": 0,
            "cache_misses": 0,
            "inference_count": 0,
            "total_inference_time": 0.0,
        }

        print(f"ğŸš€ å„ªåŒ–ç‰ˆ BERT æ¨è–¦å™¨åˆå§‹åŒ–")
        print(f"  â”œâ”€ è¨­å‚™: {self.device}")
        print(f"  â”œâ”€ FP16: {'å•Ÿç”¨' if self.use_fp16 else 'åœç”¨'}")
        print(f"  â”œâ”€ æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        print(f"  â””â”€ å¿«å–æœŸé™: {self.cache_expiry_days} å¤©")

        # è¼‰å…¥è³‡æº
        if model_path:
            self.load_model(model_path)
        if dataset_path:
            self.load_dataset(dataset_path)
        if anime_metadata_path:
            self.load_anime_metadata(anime_metadata_path)

    def load_model(self, model_path: str) -> None:
        """è¼‰å…¥ä¸¦å„ªåŒ– BERT æ¨¡å‹"""
        try:
            print(f"\nğŸ”„ è¼‰å…¥ BERT æ¨¡å‹: {model_path}")
            logger.info(f"Loading optimized BERT model from {model_path}")

            checkpoint = torch.load(model_path, map_location=self.device)

            if isinstance(checkpoint, dict):
                if "model_state_dict" in checkpoint:
                    self.model_state = checkpoint["model_state_dict"]
                else:
                    self.model_state = checkpoint
                logger.info("Model checkpoint loaded")
            else:
                self.model = checkpoint
                self.model.to(self.device)
                self.model.eval()

                # FP16 å„ªåŒ–
                if self.use_fp16:
                    self.model = self.model.half()
                    print("  â”œâ”€ FP16 æ¨¡å¼å·²å•Ÿç”¨")

                logger.info(f"Model loaded to {self.device}")

            print("âœ… BERT æ¨¡å‹è¼‰å…¥å®Œæˆï¼")

        except Exception as e:
            logger.error(f"Failed to load BERT model: {e}")
            raise

    def load_dataset(self, dataset_path: str) -> None:
        """è¼‰å…¥è³‡æ–™é›†"""
        try:
            print(f"\nğŸ”„ è¼‰å…¥è³‡æ–™é›†: {dataset_path}")
            import pickle

            with open(dataset_path, "rb") as f:
                data = pickle.load(f)

            # è™•ç†å­—å…¸æ ¼å¼çš„æ˜ å°„æª”æ¡ˆ
            if isinstance(data, dict):
                if "item_to_idx" in data and "idx_to_item" in data:
                    # æ–°æ ¼å¼ï¼šç›´æ¥ä½¿ç”¨ item_to_idx å’Œ idx_to_item
                    self.id_mapping = data["item_to_idx"]
                    self.reverse_id_mapping = data["idx_to_item"]
                    self.num_items = data.get("num_items", len(self.id_mapping))
                else:
                    # èˆŠæ ¼å¼ï¼šdata æœ¬èº«å°±æ˜¯æ˜ å°„
                    self.id_mapping = data
                    self.reverse_id_mapping = {v: k for k, v in data.items()}
                    self.num_items = len(self.id_mapping)
            elif hasattr(data, "smap"):
                # ç‰©ä»¶æ ¼å¼
                self.id_mapping = data.smap
                self.reverse_id_mapping = {v: k for k, v in self.id_mapping.items()}
                self.num_items = len(self.id_mapping)
            else:
                raise ValueError(f"Unknown dataset format: {type(data)}")

            logger.info(f"Dataset loaded with {len(self.id_mapping)} items")
            print(f"âœ… è³‡æ–™é›†è¼‰å…¥å®Œæˆï¼å…± {len(self.id_mapping)} å€‹é …ç›®")

        except Exception as e:
            logger.error(f"Failed to load dataset: {e}")
            raise

    def load_anime_metadata(self, metadata_path: str) -> None:
        """è¼‰å…¥å‹•ç•« metadata"""
        try:
            print(f"\nğŸ”„ è¼‰å…¥å‹•ç•« Metadata: {metadata_path}")

            with open(metadata_path, "r", encoding="utf-8") as f:
                metadata = json.load(f)

            if isinstance(metadata, list):
                self.anime_metadata = {
                    item.get("id") or item.get("anime_id"): item for item in metadata
                }
            else:
                self.anime_metadata = metadata

            logger.info(f"Loaded metadata for {len(self.anime_metadata)} anime")
            print(f"âœ… Metadata è¼‰å…¥å®Œæˆï¼å…± {len(self.anime_metadata)} éƒ¨å‹•ç•«")

        except Exception as e:
            logger.error(f"Failed to load anime metadata: {e}")
            self.anime_metadata = {}

    def _compute_profile_hash(self, anime_ids: List[int]) -> str:
        """è¨ˆç®—ä½¿ç”¨è€… profile çš„ hash å€¼"""
        # æ’åºå¾Œè¨ˆç®— hashï¼Œç¢ºä¿ç›¸åŒåˆ—è¡¨ç”¢ç”Ÿç›¸åŒ hash
        sorted_ids = sorted(anime_ids)
        hash_input = ",".join(map(str, sorted_ids))
        return hashlib.sha256(hash_input.encode()).hexdigest()[:16]

    def get_cached_profile(
        self, username: str, current_anime_ids: List[int]
    ) -> Optional[Dict[str, Any]]:
        """
        å¾è³‡æ–™åº«å–å¾—å¿«å–çš„ä½¿ç”¨è€… Profile

        Args:
            username: AniList ä½¿ç”¨è€…åç¨±
            current_anime_ids: ç•¶å‰ä½¿ç”¨è€…çš„å‹•ç•«åˆ—è¡¨

        Returns:
            å¿«å–çš„ Profileï¼Œå¦‚æœä¸å­˜åœ¨æˆ–å·²éæœŸå‰‡è¿”å› None
        """
        if not self.db_session:
            return None

        try:
            current_hash = self._compute_profile_hash(current_anime_ids)

            statement = select(BERTUserProfile).where(
                BERTUserProfile.anilist_username == username,
                BERTUserProfile.profile_hash == current_hash,
            )
            profile = self.db_session.exec(statement).first()

            if profile:
                # æª¢æŸ¥æ˜¯å¦éæœŸ
                age = datetime.utcnow() - profile.updated_at
                if age.days < self.cache_expiry_days:
                    self.stats["cache_hits"] += 1
                    print(f"ğŸ’¾ ä½¿ç”¨å¿«å–çš„ Profile: {username} (å¹´é½¡: {age.days} å¤©)")
                    logger.info(f"Cache HIT for user {username}")

                    return {
                        "bert_features": json.loads(profile.bert_features),
                        "anime_count": profile.anime_count,
                        "updated_at": profile.updated_at,
                    }
                else:
                    print(f"â° Profile å¿«å–å·²éæœŸ ({age.days} å¤©)")
                    logger.info(f"Cache EXPIRED for user {username}")

            self.stats["cache_misses"] += 1
            return None

        except Exception as e:
            logger.error(f"Error reading cached profile: {e}")
            return None

    def save_profile_cache(
        self,
        username: str,
        anilist_id: int,
        anime_ids: List[int],
        bert_features: Dict[str, Any],
    ) -> None:
        """
        å„²å­˜ä½¿ç”¨è€… Profile åˆ°è³‡æ–™åº«

        Args:
            username: AniList ä½¿ç”¨è€…åç¨±
            anilist_id: AniList ä½¿ç”¨è€… ID
            anime_ids: ä½¿ç”¨è€…çš„å‹•ç•« ID åˆ—è¡¨
            bert_features: BERT æå–çš„ç‰¹å¾µ
        """
        if not self.db_session:
            return

        try:
            profile_hash = self._compute_profile_hash(anime_ids)

            # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
            statement = select(BERTUserProfile).where(
                BERTUserProfile.anilist_username == username
            )
            existing = self.db_session.exec(statement).first()

            if existing:
                # æ›´æ–°ç¾æœ‰ Profile
                existing.user_anime_ids = json.dumps(anime_ids)
                existing.bert_features = json.dumps(bert_features, ensure_ascii=False)
                existing.profile_hash = profile_hash
                existing.updated_at = datetime.utcnow()
                existing.anime_count = len(anime_ids)
                print(f"ğŸ”„ æ›´æ–° Profile å¿«å–: {username}")
            else:
                # æ–°å¢ Profile
                profile = BERTUserProfile(
                    anilist_username=username,
                    anilist_id=anilist_id,
                    user_anime_ids=json.dumps(anime_ids),
                    bert_features=json.dumps(bert_features, ensure_ascii=False),
                    profile_hash=profile_hash,
                    anime_count=len(anime_ids),
                )
                self.db_session.add(profile)
                print(f"ğŸ’¾ å„²å­˜ Profile å¿«å–: {username}")

            self.db_session.commit()
            logger.info(f"Profile cached for user {username}")

        except Exception as e:
            self.db_session.rollback()
            logger.error(f"Error saving profile cache: {e}")
            print(f"âš ï¸ Profile å¿«å–å„²å­˜å¤±æ•—: {e}")

    def get_cached_recommendations(
        self, username: str, profile_hash: str, top_k: int = 50
    ) -> Optional[List[Dict[str, Any]]]:
        """
        å¾è³‡æ–™åº«å–å¾—å¿«å–çš„æ¨è–¦çµæœ

        Args:
            username: AniList ä½¿ç”¨è€…åç¨±
            profile_hash: Profile hash å€¼
            top_k: éœ€è¦çš„æ¨è–¦æ•¸é‡

        Returns:
            å¿«å–çš„æ¨è–¦åˆ—è¡¨ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–å·²éæœŸå‰‡è¿”å› None
        """
        if not self.db_session:
            return None

        try:
            statement = select(BERTRecommendationCache).where(
                BERTRecommendationCache.anilist_username == username,
                BERTRecommendationCache.profile_hash == profile_hash,
                BERTRecommendationCache.top_k >= top_k,
            )
            cache = self.db_session.exec(statement).first()

            if cache:
                # æª¢æŸ¥æ˜¯å¦éæœŸ
                age = datetime.utcnow() - cache.cached_at
                if age.days < self.cache_expiry_days:
                    # æ›´æ–°å‘½ä¸­æ¬¡æ•¸
                    cache.cache_hit_count += 1
                    self.db_session.commit()

                    self.stats["cache_hits"] += 1
                    print(f"ğŸ’¾ ä½¿ç”¨å¿«å–çš„æ¨è–¦: {username} (å¹´é½¡: {age.days} å¤©)")
                    logger.info(f"Recommendation cache HIT for user {username}")

                    recommendations = json.loads(cache.recommendations)
                    return recommendations[:top_k]

            self.stats["cache_misses"] += 1
            return None

        except Exception as e:
            logger.error(f"Error reading cached recommendations: {e}")
            return None

    def save_recommendations_cache(
        self,
        username: str,
        profile_hash: str,
        recommendations: List[Dict[str, Any]],
        top_k: int = 50,
    ) -> None:
        """
        å„²å­˜æ¨è–¦çµæœåˆ°è³‡æ–™åº«

        Args:
            username: AniList ä½¿ç”¨è€…åç¨±
            profile_hash: Profile hash å€¼
            recommendations: æ¨è–¦åˆ—è¡¨
            top_k: æ¨è–¦æ•¸é‡
        """
        if not self.db_session:
            return

        try:
            # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
            statement = select(BERTRecommendationCache).where(
                BERTRecommendationCache.anilist_username == username,
                BERTRecommendationCache.profile_hash == profile_hash,
            )
            existing = self.db_session.exec(statement).first()

            if existing:
                # æ›´æ–°ç¾æœ‰å¿«å–
                existing.recommendations = json.dumps(
                    recommendations, ensure_ascii=False
                )
                existing.top_k = top_k
                existing.cached_at = datetime.utcnow()
                print(f"ğŸ”„ æ›´æ–°æ¨è–¦å¿«å–: {username}")
            else:
                # æ–°å¢å¿«å–
                cache = BERTRecommendationCache(
                    anilist_username=username,
                    profile_hash=profile_hash,
                    recommendations=json.dumps(recommendations, ensure_ascii=False),
                    top_k=top_k,
                )
                self.db_session.add(cache)
                print(f"ğŸ’¾ å„²å­˜æ¨è–¦å¿«å–: {username}")

            self.db_session.commit()
            logger.info(f"Recommendations cached for user {username}")

        except Exception as e:
            self.db_session.rollback()
            logger.error(f"Error saving recommendations cache: {e}")
            print(f"âš ï¸ æ¨è–¦å¿«å–å„²å­˜å¤±æ•—: {e}")

    def get_recommendations(
        self,
        user_anime_ids: List[int],
        username: Optional[str] = None,
        anilist_id: Optional[int] = None,
        top_k: int = 50,
        use_anilist_ids: bool = True,
        force_refresh: bool = False,
    ) -> List[Dict[str, Any]]:
        """
        ç²å–æ¨è–¦ï¼ˆå„ªåŒ–ç‰ˆï¼Œæ”¯æ´å¿«å–ï¼‰

        Args:
            user_anime_ids: ä½¿ç”¨è€…è§€çœ‹éçš„å‹•ç•« ID åˆ—è¡¨
            username: AniList ä½¿ç”¨è€…åç¨±ï¼ˆç”¨æ–¼å¿«å–ï¼‰
            anilist_id: AniList ä½¿ç”¨è€… ID
            top_k: è¿”å›å‰ K å€‹æ¨è–¦
            use_anilist_ids: è¼¸å…¥æ˜¯å¦ç‚º AniList ID
            force_refresh: å¼·åˆ¶é‡æ–°è¨ˆç®—ï¼Œå¿½ç•¥å¿«å–

        Returns:
            æ¨è–¦å‹•ç•«åˆ—è¡¨
        """
        if not user_anime_ids:
            logger.warning("Empty anime list provided")
            return []

        print("\n" + "=" * 60)
        print("ğŸ¯ å„ªåŒ–ç‰ˆ BERT æ¨è–¦å¼•æ“")
        print("=" * 60)

        # 1. å˜—è©¦ä½¿ç”¨å¿«å–
        if username and not force_refresh:
            profile_hash = self._compute_profile_hash(user_anime_ids)

            # æª¢æŸ¥æ¨è–¦å¿«å–
            cached_recs = self.get_cached_recommendations(username, profile_hash, top_k)
            if cached_recs:
                print(f"âœ… ä½¿ç”¨å¿«å–æ¨è–¦ï¼Œè·³éæ¨ç†")
                print("=" * 60 + "\n")
                return cached_recs

            print("ğŸ“‹ å¿«å–æœªå‘½ä¸­ï¼Œé–‹å§‹ BERT æ¨ç†...")

        # 2. ID æ˜ å°„ï¼ˆæ‰¹æ¬¡è™•ç†ï¼‰
        print("\nğŸ“‹ éšæ®µ 1/4: ID æ˜ å°„")
        dataset_ids = self._batch_map_ids(user_anime_ids, use_anilist_ids)

        if not dataset_ids:
            print("âŒ æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ ID")
            return []

        print(f"  âœ“ æˆåŠŸæ˜ å°„ {len(dataset_ids)}/{len(user_anime_ids)} å€‹ ID")

        # 3. BERT æ¨ç†ï¼ˆæ‰¹æ¬¡è™•ç†ï¼‰
        print("\nğŸ“‹ éšæ®µ 2/4: BERT æ¨ç†")
        recommendations = self._batch_inference(dataset_ids, top_k)

        # 4. æå–ç‰¹å¾µï¼ˆç”¨æ–¼å¿«å– Profileï¼‰
        if username and anilist_id:
            print("\nğŸ“‹ éšæ®µ 3/4: æå–ç‰¹å¾µä¸¦å¿«å– Profile")
            bert_features = self.get_anime_features(user_anime_ids, use_anilist_ids)
            self.save_profile_cache(username, anilist_id, user_anime_ids, bert_features)

            # 5. å¿«å–æ¨è–¦çµæœ
            print("\nğŸ“‹ éšæ®µ 4/4: å¿«å–æ¨è–¦çµæœ")
            profile_hash = self._compute_profile_hash(user_anime_ids)
            self.save_recommendations_cache(
                username, profile_hash, recommendations, top_k
            )

        print(f"\nğŸ‰ æ¨è–¦å®Œæˆï¼å…± {len(recommendations)} å€‹æ¨è–¦")
        print("=" * 60 + "\n")

        return recommendations

    def _batch_map_ids(self, anime_ids: List[int], use_anilist_ids: bool) -> List[int]:
        """æ‰¹æ¬¡æ˜ å°„ ID"""
        if not use_anilist_ids:
            return anime_ids

        dataset_ids = []
        for aid in anime_ids:
            if aid in self.id_mapping:
                dataset_ids.append(self.id_mapping[aid])
            elif aid in self.anime_metadata:
                meta = self.anime_metadata[aid]
                if "dataset_id" in meta:
                    dataset_ids.append(meta["dataset_id"])

        return dataset_ids

    def _batch_inference(
        self, dataset_ids: List[int], top_k: int
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹æ¬¡æ¨ç†ï¼ˆå„ªåŒ–ç‰ˆï¼‰

        Args:
            dataset_ids: è³‡æ–™é›† ID åˆ—è¡¨
            top_k: è¿”å›å‰ K å€‹æ¨è–¦

        Returns:
            æ¨è–¦åˆ—è¡¨
        """
        if self.model is None and not hasattr(self, "model_state"):
            logger.warning("Model not loaded, returning empty recommendations")
            return []

        try:
            import time

            start_time = time.time()

            # æº–å‚™è¼¸å…¥
            input_seq = self._prepare_input_sequence(dataset_ids)

            # æ¨ç†
            with torch.no_grad():
                if self.model is not None:
                    output = self.model(input_seq)
                    if isinstance(output, tuple):
                        logits = output[0]
                    else:
                        logits = output
                    scores = logits[:, -1, :].cpu().numpy()[0]
                else:
                    # Fallback: ä½¿ç”¨éš¨æ©Ÿåˆ†æ•¸
                    num_items = len(self.id_mapping) if self.id_mapping else 1000
                    scores = np.random.rand(num_items)

            # ç²å– Top-K
            top_indices = np.argsort(scores)[-top_k:][::-1]

            recommendations = []
            for idx in top_indices:
                dataset_id = int(idx)
                score = float(scores[idx])
                anilist_id = self.map_dataset_id_to_anilist_id(dataset_id)

                rec = {
                    "dataset_id": dataset_id,
                    "anilist_id": anilist_id,
                    "score": score,
                    "metadata": self.anime_metadata.get(anilist_id or dataset_id, {}),
                }
                recommendations.append(rec)

            # çµ±è¨ˆ
            inference_time = time.time() - start_time
            self.stats["inference_count"] += 1
            self.stats["total_inference_time"] += inference_time

            print(f"  âœ“ æ¨ç†å®Œæˆ ({inference_time:.2f} ç§’)")
            logger.info(f"Inference completed in {inference_time:.2f}s")

            return recommendations

        except Exception as e:
            logger.error(f"Error during batch inference: {e}")
            return []

    def _prepare_input_sequence(self, anime_ids: List[int]) -> torch.Tensor:
        """æº–å‚™æ¨¡å‹è¼¸å…¥åºåˆ—"""
        max_len = 200

        if len(anime_ids) > max_len - 2:
            anime_ids = anime_ids[-(max_len - 2) :]

        seq_tensor = torch.LongTensor([anime_ids])

        if self.use_fp16:
            return seq_tensor.to(self.device).half()
        else:
            return seq_tensor.to(self.device)

    def map_dataset_id_to_anilist_id(self, dataset_id: int) -> Optional[int]:
        """å°‡è³‡æ–™é›† ID æ˜ å°„å› AniList ID"""
        if dataset_id in self.reverse_id_mapping:
            return self.reverse_id_mapping[dataset_id]
        return None

    def get_anime_features(
        self, anime_ids: List[int], use_anilist_ids: bool = True
    ) -> Dict[str, Any]:
        """
        ç²å–å‹•ç•«ç‰¹å¾µï¼ˆæ‰¹æ¬¡è™•ç†ï¼‰

        Args:
            anime_ids: å‹•ç•« ID åˆ—è¡¨
            use_anilist_ids: æ˜¯å¦ä½¿ç”¨ AniList ID

        Returns:
            èšåˆçš„ç‰¹å¾µå­—å…¸
        """
        from collections import Counter

        features = {
            "genres": Counter(),
            "tags": Counter(),
            "studios": Counter(),
            "formats": Counter(),
            "seasons": Counter(),
        }

        for aid in anime_ids:
            metadata = (
                self.anime_metadata.get(aid, {})
                if use_anilist_ids
                else self.anime_metadata.get(self.map_dataset_id_to_anilist_id(aid), {})
            )

            # æ‰¹æ¬¡è™•ç†ç‰¹å¾µæå–
            if "genres" in metadata:
                features["genres"].update(metadata["genres"])

            if "tags" in metadata:
                for tag in metadata["tags"]:
                    tag_name = tag if isinstance(tag, str) else tag.get("name", "")
                    if tag_name:
                        features["tags"][tag_name] += 1

            if "studios" in metadata:
                for studio in metadata["studios"]:
                    studio_name = (
                        studio if isinstance(studio, str) else studio.get("name", "")
                    )
                    if studio_name:
                        features["studios"][studio_name] += 1

            if "format" in metadata:
                features["formats"][metadata["format"]] += 1

            if "season" in metadata:
                features["seasons"][metadata["season"]] += 1

        return {k: dict(v) for k, v in features.items()}

    def get_stats(self) -> Dict[str, Any]:
        """å–å¾—æ•ˆèƒ½çµ±è¨ˆ"""
        avg_inference_time = (
            self.stats["total_inference_time"] / self.stats["inference_count"]
            if self.stats["inference_count"] > 0
            else 0
        )

        total_requests = self.stats["cache_hits"] + self.stats["cache_misses"]
        cache_hit_rate = (
            self.stats["cache_hits"] / total_requests if total_requests > 0 else 0
        )

        return {
            "cache_hits": self.stats["cache_hits"],
            "cache_misses": self.stats["cache_misses"],
            "cache_hit_rate": f"{cache_hit_rate:.1%}",
            "inference_count": self.stats["inference_count"],
            "avg_inference_time": f"{avg_inference_time:.2f}s",
            "device": str(self.device),
            "fp16_enabled": self.use_fp16,
        }

    def print_stats(self) -> None:
        """åˆ—å°æ•ˆèƒ½çµ±è¨ˆ"""
        stats = self.get_stats()
        print("\n" + "=" * 60)
        print("ğŸ“Š BERT æ¨è–¦å™¨æ•ˆèƒ½çµ±è¨ˆ")
        print("=" * 60)
        print(f"  å¿«å–å‘½ä¸­: {stats['cache_hits']}")
        print(f"  å¿«å–æœªå‘½ä¸­: {stats['cache_misses']}")
        print(f"  å¿«å–å‘½ä¸­ç‡: {stats['cache_hit_rate']}")
        print(f"  æ¨ç†æ¬¡æ•¸: {stats['inference_count']}")
        print(f"  å¹³å‡æ¨ç†æ™‚é–“: {stats['avg_inference_time']}")
        print(f"  è¨­å‚™: {stats['device']}")
        print(f"  FP16: {'å•Ÿç”¨' if stats['fp16_enabled'] else 'åœç”¨'}")
        print("=" * 60 + "\n")

    def clear_user_cache(self, username: str) -> None:
        """æ¸…é™¤ç‰¹å®šä½¿ç”¨è€…çš„å¿«å–"""
        if not self.db_session:
            return

        try:
            # æ¸…é™¤ Profile
            statement = select(BERTUserProfile).where(
                BERTUserProfile.anilist_username == username
            )
            profile = self.db_session.exec(statement).first()
            if profile:
                self.db_session.delete(profile)

            # æ¸…é™¤æ¨è–¦
            statement = select(BERTRecommendationCache).where(
                BERTRecommendationCache.anilist_username == username
            )
            caches = self.db_session.exec(statement).all()
            for cache in caches:
                self.db_session.delete(cache)

            self.db_session.commit()
            print(f"ğŸ—‘ï¸ å·²æ¸…é™¤ä½¿ç”¨è€… {username} çš„å¿«å–")
            logger.info(f"Cleared cache for user {username}")

        except Exception as e:
            self.db_session.rollback()
            logger.error(f"Error clearing cache: {e}")

    def is_available(self) -> bool:
        """æª¢æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        return self.model is not None or hasattr(self, "model_state")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # æ¸¬è©¦åˆå§‹åŒ–
    recommender = OptimizedBERTRecommender(device="auto", use_fp16=True, batch_size=32)
    print(f"\nå„ªåŒ–ç‰ˆ BERT æ¨è–¦å™¨åˆå§‹åŒ–å®Œæˆ")
    print(f"æ¨¡å‹å¯ç”¨: {recommender.is_available()}")
    recommender.print_stats()
