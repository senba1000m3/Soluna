"""
BERT-based Anime Recommender Wrapper
æ•´åˆ AnimeRecBERT æ¨¡å‹ç”¨æ–¼å‹•ç•«æ¨è–¦
"""

import json
import logging
import pickle
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np
import torch
import torch.nn as nn
from tqdm import tqdm

logger = logging.getLogger(__name__)


class BERTRecommender:
    """
    AnimeRecBERT æ¨¡å‹åŒ…è£å™¨
    è² è²¬è¼‰å…¥é è¨“ç·´æ¨¡å‹ä¸¦é€²è¡Œæ¨è–¦
    """

    def __init__(
        self,
        model_path: Optional[str] = None,
        dataset_path: Optional[str] = None,
        anime_metadata_path: Optional[str] = None,
        device: str = "cpu",
    ):
        """
        åˆå§‹åŒ– BERT æ¨è–¦å™¨

        Args:
            model_path: é è¨“ç·´æ¨¡å‹è·¯å¾‘ (.pth æª”æ¡ˆ)
            dataset_path: è³‡æ–™é›†è·¯å¾‘ (.pkl æª”æ¡ˆï¼ŒåŒ…å« ID æ˜ å°„)
            anime_metadata_path: å‹•ç•« metadata è·¯å¾‘ (.json æª”æ¡ˆ)
            device: é‹ç®—è¨­å‚™ ('cpu' æˆ– 'cuda')
        """
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        self.model = None
        self.dataset = None
        self.anime_metadata = {}
        self.id_mapping = {}  # dataset_id -> anime_info
        self.reverse_id_mapping = {}  # external_id -> dataset_id

        if model_path:
            self.load_model(model_path)
        if dataset_path:
            self.load_dataset(dataset_path)
        if anime_metadata_path:
            self.load_anime_metadata(anime_metadata_path)

    def load_model(self, model_path: str) -> None:
        """
        è¼‰å…¥é è¨“ç·´çš„ BERT æ¨¡å‹

        Args:
            model_path: æ¨¡å‹æª”æ¡ˆè·¯å¾‘
        """
        try:
            print(f"ğŸ”„ æ­£åœ¨è¼‰å…¥ BERT æ¨¡å‹: {model_path}")
            logger.info(f"Loading BERT model from {model_path}")

            with tqdm(total=100, desc="è¼‰å…¥æ¨¡å‹", unit="%") as pbar:
                checkpoint = torch.load(model_path, map_location=self.device)
                pbar.update(50)

                # æ ¹æ“š checkpoint çµæ§‹è¼‰å…¥æ¨¡å‹
                if isinstance(checkpoint, dict):
                    # å¦‚æœæœ‰ model_state_dict
                    if "model_state_dict" in checkpoint:
                        state_dict = checkpoint["model_state_dict"]
                    else:
                        state_dict = checkpoint

                    # é€™è£¡éœ€è¦å¯¦éš›çš„æ¨¡å‹æ¶æ§‹
                    # æš«æ™‚å„²å­˜ state_dictï¼Œå¯¦éš›ä½¿ç”¨æ™‚éœ€è¦å®Œæ•´çš„æ¨¡å‹å®šç¾©
                    self.model_state = state_dict
                    logger.info("Model checkpoint loaded successfully")
                    pbar.update(30)
                else:
                    self.model = checkpoint
                    self.model.to(self.device)
                    self.model.eval()
                    logger.info("Model loaded and moved to device")
                    pbar.update(30)

                pbar.update(20)

            print("âœ… BERT æ¨¡å‹è¼‰å…¥å®Œæˆï¼")

        except Exception as e:
            logger.error(f"Failed to load BERT model: {e}")
            raise

    def load_dataset(self, dataset_path: str) -> None:
        """
        è¼‰å…¥è³‡æ–™é›†å’Œ ID æ˜ å°„è³‡è¨Š

        Args:
            dataset_path: è³‡æ–™é›† pickle æª”æ¡ˆè·¯å¾‘
        """
        try:
            print(f"ğŸ”„ æ­£åœ¨è¼‰å…¥è³‡æ–™é›†: {dataset_path}")
            logger.info(f"Loading dataset from {dataset_path}")

            with tqdm(total=100, desc="è¼‰å…¥è³‡æ–™é›†", unit="%") as pbar:
                with open(dataset_path, "rb") as f:
                    self.dataset = pickle.load(f)
                pbar.update(70)

                # å»ºç«‹ ID æ˜ å°„
                if hasattr(self.dataset, "smap"):
                    # smap: item_id -> sequential_id
                    self.id_mapping = self.dataset.smap
                    self.reverse_id_mapping = {v: k for k, v in self.id_mapping.items()}
                pbar.update(30)

            logger.info(f"Dataset loaded with {len(self.id_mapping)} items")
            print(f"âœ… è³‡æ–™é›†è¼‰å…¥å®Œæˆï¼å…± {len(self.id_mapping)} å€‹é …ç›®")

        except Exception as e:
            logger.error(f"Failed to load dataset: {e}")
            raise

    def load_anime_metadata(self, metadata_path: str) -> None:
        """
        è¼‰å…¥å‹•ç•« metadata (genres, tags, etc.)

        Args:
            metadata_path: Metadata JSON æª”æ¡ˆè·¯å¾‘
        """
        try:
            print(f"ğŸ”„ æ­£åœ¨è¼‰å…¥å‹•ç•« Metadata: {metadata_path}")
            logger.info(f"Loading anime metadata from {metadata_path}")

            with tqdm(total=100, desc="è¼‰å…¥ Metadata", unit="%") as pbar:
                with open(metadata_path, "r", encoding="utf-8") as f:
                    metadata = json.load(f)
                pbar.update(80)

                # å‡è¨­ metadata æ˜¯ list of dicts æˆ– dict
                if isinstance(metadata, list):
                    self.anime_metadata = {
                        item.get("id") or item.get("anime_id"): item
                        for item in metadata
                    }
                else:
                    self.anime_metadata = metadata
                pbar.update(20)

            logger.info(f"Loaded metadata for {len(self.anime_metadata)} anime")
            print(f"âœ… Metadata è¼‰å…¥å®Œæˆï¼å…± {len(self.anime_metadata)} éƒ¨å‹•ç•«")

        except Exception as e:
            logger.error(f"Failed to load anime metadata: {e}")
            # ä¸æ˜¯è‡´å‘½éŒ¯èª¤ï¼Œç¹¼çºŒåŸ·è¡Œ
            self.anime_metadata = {}

    def map_anilist_id_to_dataset_id(self, anilist_id: int) -> Optional[int]:
        """
        å°‡ AniList ID æ˜ å°„åˆ°è³‡æ–™é›† ID

        Args:
            anilist_id: AniList å‹•ç•« ID

        Returns:
            è³‡æ–™é›†ä¸­çš„å°æ‡‰ IDï¼Œå¦‚æœæ‰¾ä¸åˆ°å‰‡è¿”å› None
        """
        # é€™è£¡éœ€è¦å¯¦éš›çš„æ˜ å°„é‚è¼¯
        # å¯èƒ½éœ€è¦é¡å¤–çš„æ˜ å°„æª”æ¡ˆ (anilist_id -> dataset_id)
        # æš«æ™‚å‡è¨­ ID ç›¸åŒæˆ–ä½¿ç”¨ metadata é€²è¡ŒåŒ¹é…
        if anilist_id in self.id_mapping:
            return self.id_mapping[anilist_id]

        # å˜—è©¦å¾ metadata ä¸­æŸ¥æ‰¾
        if anilist_id in self.anime_metadata:
            # å¦‚æœ metadata ä¸­æœ‰æ˜ å°„è³‡è¨Š
            meta = self.anime_metadata[anilist_id]
            if "dataset_id" in meta:
                return meta["dataset_id"]

        return None

    def map_dataset_id_to_anilist_id(self, dataset_id: int) -> Optional[int]:
        """
        å°‡è³‡æ–™é›† ID æ˜ å°„å› AniList ID

        Args:
            dataset_id: è³‡æ–™é›†ä¸­çš„å‹•ç•« ID

        Returns:
            å°æ‡‰çš„ AniList IDï¼Œå¦‚æœæ‰¾ä¸åˆ°å‰‡è¿”å› None
        """
        if dataset_id in self.reverse_id_mapping:
            return self.reverse_id_mapping[dataset_id]
        return None

    def get_recommendations(
        self, user_anime_ids: List[int], top_k: int = 50, use_anilist_ids: bool = True
    ) -> List[Dict[str, Any]]:
        """
        åŸºæ–¼ä½¿ç”¨è€…è§€çœ‹æ­·å²ç²å–æ¨è–¦

        Args:
            user_anime_ids: ä½¿ç”¨è€…è§€çœ‹éçš„å‹•ç•« ID åˆ—è¡¨
            top_k: è¿”å›å‰ K å€‹æ¨è–¦
            use_anilist_ids: è¼¸å…¥æ˜¯å¦ç‚º AniList ID (True) æˆ–è³‡æ–™é›† ID (False)

        Returns:
            æ¨è–¦å‹•ç•«åˆ—è¡¨ï¼Œæ¯å€‹åŒ…å« id, score, metadata
        """
        if self.model is None and not hasattr(self, "model_state"):
            logger.warning("Model not loaded, returning empty recommendations")
            return []

        try:
            print("\n" + "=" * 60)
            print("ğŸ¯ é–‹å§‹ç”Ÿæˆæ¨è–¦...")
            print("=" * 60)

            # 1. ID æ˜ å°„
            print("\nğŸ“‹ éšæ®µ 1/4: ID æ˜ å°„")
            if use_anilist_ids:
                dataset_ids = []
                for aid in tqdm(user_anime_ids, desc="æ˜ å°„ AniList ID", unit="å€‹"):
                    did = self.map_anilist_id_to_dataset_id(aid)
                    if did is not None:
                        dataset_ids.append(did)
                logger.info(
                    f"Mapped {len(dataset_ids)}/{len(user_anime_ids)} AniList IDs to dataset IDs"
                )
                print(f"  âœ“ æˆåŠŸæ˜ å°„ {len(dataset_ids)}/{len(user_anime_ids)} å€‹ ID")
            else:
                dataset_ids = user_anime_ids

            if not dataset_ids:
                logger.warning("No valid dataset IDs found")
                print("âŒ æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ ID")
                return []

            # 2. æº–å‚™æ¨¡å‹è¼¸å…¥
            print("\nğŸ“‹ éšæ®µ 2/4: æº–å‚™æ¨¡å‹è¼¸å…¥")
            # é€™è£¡éœ€è¦æ ¹æ“šå¯¦éš›çš„ BERT4Rec è¼¸å…¥æ ¼å¼ä¾†æº–å‚™
            # é€šå¸¸æ˜¯ä¸€å€‹åºåˆ—çš„ token IDs
            with tqdm(total=100, desc="æº–å‚™è¼¸å…¥åºåˆ—", unit="%") as pbar:
                input_seq = self._prepare_input_sequence(dataset_ids)
                pbar.update(100)

            # 3. æ¨¡å‹æ¨ç†
            print("\nğŸ“‹ éšæ®µ 3/4: æ¨¡å‹æ¨ç†")
            scores = self._inference(input_seq)
            print("  âœ“ æ¨ç†å®Œæˆ")

            # 4. ç²å– Top-K æ¨è–¦
            print(f"\nğŸ“‹ éšæ®µ 4/4: ç”Ÿæˆ Top-{top_k} æ¨è–¦")
            top_indices = np.argsort(scores)[-top_k:][::-1]
            recommendations = []

            for idx in tqdm(top_indices, desc="è™•ç†æ¨è–¦çµæœ", unit="å€‹"):
                dataset_id = idx
                score = float(scores[idx])

                # æ˜ å°„å› AniList ID
                anilist_id = self.map_dataset_id_to_anilist_id(dataset_id)

                rec = {
                    "dataset_id": int(dataset_id),
                    "anilist_id": anilist_id,
                    "score": score,
                    "metadata": self.anime_metadata.get(anilist_id or dataset_id, {}),
                }
                recommendations.append(rec)

            print(f"\nğŸ‰ æ¨è–¦ç”Ÿæˆå®Œæˆï¼å…± {len(recommendations)} å€‹æ¨è–¦")
            print("=" * 60 + "\n")
            return recommendations

        except Exception as e:
            logger.error(f"Error during recommendation: {e}")
            return []

    def _prepare_input_sequence(self, anime_ids: List[int]) -> torch.Tensor:
        """
        æº–å‚™æ¨¡å‹è¼¸å…¥åºåˆ—

        Args:
            anime_ids: å‹•ç•« ID åˆ—è¡¨

        Returns:
            æ¨¡å‹è¼¸å…¥å¼µé‡
        """
        # BERT4Rec è¼¸å…¥æ ¼å¼ï¼š[CLS] item1 item2 ... itemN [MASK]
        # é€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›éœ€è¦æ ¹æ“šæ¨¡å‹è¨“ç·´æ™‚çš„æ ¼å¼
        max_len = 200  # å‡è¨­æœ€å¤§åºåˆ—é•·åº¦ç‚º 200

        # å¡«å……æˆ–æˆªæ–·
        if len(anime_ids) > max_len - 2:
            anime_ids = anime_ids[-(max_len - 2) :]

        # æ·»åŠ ç‰¹æ®Š token (å¦‚æœéœ€è¦)
        # seq = [CLS_TOKEN] + anime_ids + [MASK_TOKEN]

        # è½‰æ›ç‚º tensor
        seq_tensor = torch.LongTensor([anime_ids])
        return seq_tensor.to(self.device)

    def _inference(self, input_seq: torch.Tensor) -> np.ndarray:
        """
        åŸ·è¡Œæ¨¡å‹æ¨ç†

        Args:
            input_seq: è¼¸å…¥åºåˆ—å¼µé‡

        Returns:
            æ‰€æœ‰å‹•ç•«çš„è©•åˆ†é™£åˆ—
        """
        if self.model is None:
            # å¦‚æœæ¨¡å‹æœªå®Œå…¨è¼‰å…¥ï¼Œè¿”å›éš¨æ©Ÿåˆ†æ•¸ï¼ˆç”¨æ–¼æ¸¬è©¦ï¼‰
            logger.warning("Model not fully loaded, returning dummy scores")
            num_items = len(self.id_mapping) if self.id_mapping else 1000
            print("  âš ï¸  ä½¿ç”¨æ¨¡æ“¬åˆ†æ•¸ï¼ˆæ¨¡å‹æœªå®Œå…¨è¼‰å…¥ï¼‰")
            return np.random.rand(num_items)

        try:
            with torch.no_grad():
                with tqdm(total=100, desc="æ¨¡å‹æ¨ç†ä¸­", unit="%") as pbar:
                    # åŸ·è¡Œå‰å‘å‚³æ’­
                    output = self.model(input_seq)
                    pbar.update(70)

                    # ç²å–æœ€å¾Œä¸€å€‹ä½ç½®çš„é æ¸¬ï¼ˆMASK ä½ç½®ï¼‰
                    if isinstance(output, tuple):
                        logits = output[0]
                    else:
                        logits = output

                    # å–æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥çš„è¼¸å‡º
                    scores = logits[:, -1, :].cpu().numpy()[0]
                    pbar.update(30)

                return scores

        except Exception as e:
            logger.error(f"Error during inference: {e}")
            # è¿”å›éš¨æ©Ÿåˆ†æ•¸ä½œç‚º fallback
            num_items = len(self.id_mapping) if self.id_mapping else 1000
            return np.random.rand(num_items)

    def get_anime_features(
        self, anime_ids: List[int], use_anilist_ids: bool = True
    ) -> Dict[str, Any]:
        """
        ç²å–ä¸€çµ„å‹•ç•«çš„èšåˆç‰¹å¾µ

        Args:
            anime_ids: å‹•ç•« ID åˆ—è¡¨
            use_anilist_ids: æ˜¯å¦ä½¿ç”¨ AniList ID

        Returns:
            èšåˆçš„ç‰¹å¾µå­—å…¸ï¼ˆgenres, tags, studios ç­‰çš„åˆ†å¸ƒï¼‰
        """
        from collections import Counter

        features = {
            "genres": Counter(),
            "tags": Counter(),
            "studios": Counter(),
            "formats": Counter(),
            "seasons": Counter(),
        }

        for aid in tqdm(anime_ids, desc="æå–å‹•ç•«ç‰¹å¾µ", unit="éƒ¨"):
            # ç²å– metadata
            if use_anilist_ids:
                metadata = self.anime_metadata.get(aid, {})
            else:
                anilist_id = self.map_dataset_id_to_anilist_id(aid)
                metadata = self.anime_metadata.get(anilist_id, {}) if anilist_id else {}

            # èšåˆç‰¹å¾µ
            if "genres" in metadata:
                for genre in metadata["genres"]:
                    features["genres"][genre] += 1

            if "tags" in metadata:
                for tag in metadata["tags"]:
                    tag_name = tag if isinstance(tag, str) else tag.get("name", "")
                    if tag_name:
                        features["tags"][tag_name] += 1

            if "studios" in metadata:
                for studio in metadata["studios"]:
                    studio_name = (
                        studio if isinstance(studio, str) else studio.get("name", "")
                    )
                    if studio_name:
                        features["studios"][studio_name] += 1

            if "format" in metadata:
                features["formats"][metadata["format"]] += 1

            if "season" in metadata:
                features["seasons"][metadata["season"]] += 1

        # è½‰æ›ç‚ºæ™®é€š dict ä¸¦æ­£è¦åŒ–
        return {k: dict(v) for k, v in features.items()}

    def is_available(self) -> bool:
        """
        æª¢æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨

        Returns:
            æ¨¡å‹æ˜¯å¦å·²è¼‰å…¥ä¸”å¯ç”¨
        """
        return self.model is not None or hasattr(self, "model_state")


# ç°¡å–®çš„æ¸¬è©¦å‡½æ•¸
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # æ¸¬è©¦åˆå§‹åŒ–
    recommender = BERTRecommender()
    print(f"BERT Recommender initialized. Available: {recommender.is_available()}")

    # æ¸¬è©¦åŠŸèƒ½ï¼ˆéœ€è¦å¯¦éš›çš„æ¨¡å‹æª”æ¡ˆï¼‰
    # recommender = BERTRecommender(
    #     model_path="path/to/model.pth",
    #     dataset_path="path/to/dataset.pkl",
    #     anime_metadata_path="path/to/animes.json"
    # )
